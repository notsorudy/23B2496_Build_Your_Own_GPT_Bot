{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS2cXzpj4kAY"
      },
      "source": [
        "**Requirements for Installing Gensim**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "1-XhsYX9R9sF",
        "outputId": "f93cced7-c638-4041-a0fe-39037d01fab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: gensim 4.3.1\n",
            "Uninstalling gensim-4.3.1:\n",
            "  Successfully uninstalled gensim-4.3.1\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Found existing installation: numpy 1.24.4\n",
            "Uninstalling numpy-1.24.4:\n",
            "  Successfully uninstalled numpy-1.24.4\n",
            "Collecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting gensim==4.3.1\n",
            "  Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.1) (1.17.2)\n",
            "Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "Installing collected packages: numpy, scipy, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.1 numpy-1.24.4 scipy-1.10.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9cc1b423457943e69816cebf4e348581",
              "pip_warning": {
                "packages": [
                  "gensim",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y gensim scipy numpy\n",
        "!pip install numpy==1.24.4 scipy==1.10.1 gensim==4.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afNo-qx74PD0"
      },
      "source": [
        "**Importing Important Libraries like Genism, Pandas...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "da4jn82zXXcR"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqzb_Bp64P_3"
      },
      "source": [
        "**Importing NLTK and downloading all necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFY5NWIx3gDH",
        "outputId": "b96cab0e-8897-48d1-9285-94682f00c6e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjTWxKpv4RLr"
      },
      "source": [
        "**Reading the Json File and Converting it to CSV format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MkdZBHvUzc5N"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "File /content/News_Category_Dataset_v3.json does not exist",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/content/News_Category_Dataset_v3.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.to_csv(\u001b[33m\"\u001b[39m\u001b[33m/content/News_Category_Dataset_v3.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m data = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/content/News_Category_Dataset_v3.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
            "\u001b[31mFileNotFoundError\u001b[39m: File /content/News_Category_Dataset_v3.json does not exist"
          ]
        }
      ],
      "source": [
        "df = pd.read_json(\"/content/News_Category_Dataset_v3.json\", lines=True)\n",
        "df.to_csv(\"/content/News_Category_Dataset_v3.csv\", index=False)\n",
        "data = pd.read_csv(\"/content/News_Category_Dataset_v3.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9AR4B74Wbw"
      },
      "source": [
        "**Extracting all the Headlines, Removing NaNs and Tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uz__3yB6cwzI"
      },
      "outputs": [],
      "source": [
        "sentences = data['headline']\n",
        "\n",
        "clean_sentences = sentences.dropna().reset_index(drop=True)\n",
        "\n",
        "tokenized_sentences = [word_tokenize(str(sentence)) for sentence in clean_sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHg91YJg4Xnp"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "veAePpNJtvVk"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_sentences = []\n",
        "for sentence in tokenized_sentences:\n",
        "  lowered = [word.lower() for word in sentence]\n",
        "  tagged = pos_tag(lowered)\n",
        "  lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "  lemmatized_sentences.append(lemmatized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REtRuV5t4ZD7"
      },
      "source": [
        "**Applying StopWords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z0H1vHyWJ1oD"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentences = []\n",
        "for sentence in lemmatized_sentences:\n",
        "    filtered = [word for word in sentence if word not in stop_words]\n",
        "    filtered_sentences.append(filtered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beUaAc284dWp"
      },
      "source": [
        "**Downloading Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k1e5nLlBTur5"
      },
      "outputs": [],
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt0-RdIG4bb7"
      },
      "source": [
        "**Keeping only the Sentences which are not Empty and applying Average Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RE0sWdsLUuyI"
      },
      "outputs": [],
      "source": [
        "filtered_sentences_cleaned = []\n",
        "vectorized_sentences = []\n",
        "\n",
        "for sentence in filtered_sentences:\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:  # for only sentences that dont have NA\n",
        "        avg_vector = np.mean(sentence_vectors, axis=0)\n",
        "        vectorized_sentences.append(avg_vector)\n",
        "        filtered_sentences_cleaned.append(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5uGnqlk4eiP"
      },
      "source": [
        "**Creating a Function to convert Input Sentence List into a Vector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p05Uzf45ex0g"
      },
      "outputs": [],
      "source": [
        "def get_sentence_vector(sentence, wv):\n",
        "\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:\n",
        "        return np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdE82ctC4e_x"
      },
      "source": [
        "**Creating a Function to Find the Top 5 most Similar Sentences from the Dataset based on the Input Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jiqJNa0gfc-s"
      },
      "outputs": [],
      "source": [
        "def find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences, k=5):\n",
        "    vec = get_sentence_vector(input_sentence, wv)\n",
        "    if vec is None:\n",
        "        return\n",
        "\n",
        "    similarities = cosine_similarity(vec, np.array(vectorized_sentences))[0]\n",
        "    top_k_idx = similarities.argsort()[::-1][:k]\n",
        "\n",
        "    return [(filtered_sentences[i], similarities[i]) for i in top_k_idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiTBPHCf4f15"
      },
      "source": [
        "**Example Input Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6CkZYTzvu1d",
        "outputId": "9e3b179b-578d-4bca-a978-b8530f75cdc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. guy 's running president want give 'free ' money (Similarity: 0.7809)\n",
            "2. copycat chick-fil-a sandwich recipe ( hungry sunday ) (Similarity: 0.6981)\n",
            "3. clothe organization : family 's closet say ( photo ) (Similarity: 0.6963)\n",
            "4. 'la la land ' win bafta 's top prize , continue hot streak road oscar (Similarity: 0.6809)\n",
            "5. guy 's get 2 word president , 's put d.c . (Similarity: 0.6600)\n"
          ]
        }
      ],
      "source": [
        "input_sentence = [\"president\", \"got\", \"no\", \"money\"]\n",
        "top_matches = find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences)\n",
        "\n",
        "for i, (sentence, score) in enumerate(top_matches, 1):\n",
        "    print(f\"{i}. {' '.join(sentence)} (Similarity: {score:.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
