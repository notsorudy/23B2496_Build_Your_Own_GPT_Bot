{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements for Installing Gensim**"
      ],
      "metadata": {
        "id": "CS2cXzpj4kAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y gensim scipy numpy\n",
        "!pip install numpy==1.24.4 scipy==1.10.1 gensim==4.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "1-XhsYX9R9sF",
        "outputId": "f93cced7-c638-4041-a0fe-39037d01fab3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.1\n",
            "Uninstalling gensim-4.3.1:\n",
            "  Successfully uninstalled gensim-4.3.1\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Found existing installation: numpy 1.24.4\n",
            "Uninstalling numpy-1.24.4:\n",
            "  Successfully uninstalled numpy-1.24.4\n",
            "Collecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting gensim==4.3.1\n",
            "  Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.1) (1.17.2)\n",
            "Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Using cached gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "Installing collected packages: numpy, scipy, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.1 numpy-1.24.4 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim",
                  "numpy"
                ]
              },
              "id": "9cc1b423457943e69816cebf4e348581"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Important Libraries like Genism, Pandas...**"
      ],
      "metadata": {
        "id": "afNo-qx74PD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "da4jn82zXXcR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing NLTK and downloading all necessary packages**"
      ],
      "metadata": {
        "id": "Jqzb_Bp64P_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFY5NWIx3gDH",
        "outputId": "b96cab0e-8897-48d1-9285-94682f00c6e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the Json File and Converting it to CSV format**"
      ],
      "metadata": {
        "id": "ZjTWxKpv4RLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MkdZBHvUzc5N"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"/content/News_Category_Dataset_v3.json\", lines=True)\n",
        "df.to_csv(\"/content/News_Category_Dataset_v3.csv\", index=False)\n",
        "data = pd.read_csv(\"/content/News_Category_Dataset_v3.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting all the Headlines, Removing NaNs and Tokenizing**"
      ],
      "metadata": {
        "id": "_H9AR4B74Wbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data['headline']\n",
        "\n",
        "clean_sentences = sentences.dropna().reset_index(drop=True)\n",
        "\n",
        "tokenized_sentences = [word_tokenize(str(sentence)) for sentence in clean_sentences]"
      ],
      "metadata": {
        "id": "Uz__3yB6cwzI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "IHg91YJg4Xnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_sentences = []\n",
        "for sentence in tokenized_sentences:\n",
        "  lowered = [word.lower() for word in sentence]\n",
        "  tagged = pos_tag(lowered)\n",
        "  lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "  lemmatized_sentences.append(lemmatized)\n"
      ],
      "metadata": {
        "id": "veAePpNJtvVk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying StopWords**"
      ],
      "metadata": {
        "id": "REtRuV5t4ZD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentences = []\n",
        "for sentence in lemmatized_sentences:\n",
        "    filtered = [word for word in sentence if word not in stop_words]\n",
        "    filtered_sentences.append(filtered)\n"
      ],
      "metadata": {
        "id": "z0H1vHyWJ1oD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Word2Vec**"
      ],
      "metadata": {
        "id": "beUaAc284dWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "k1e5nLlBTur5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keeping only the Sentences which are not Empty and applying Average Word2Vec**"
      ],
      "metadata": {
        "id": "Qt0-RdIG4bb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentences_cleaned = []\n",
        "vectorized_sentences = []\n",
        "\n",
        "for sentence in filtered_sentences:\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:  # for only sentences that dont have NA\n",
        "        avg_vector = np.mean(sentence_vectors, axis=0)\n",
        "        vectorized_sentences.append(avg_vector)\n",
        "        filtered_sentences_cleaned.append(sentence)\n"
      ],
      "metadata": {
        "id": "RE0sWdsLUuyI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Function to convert Input Sentence List into a Vector**"
      ],
      "metadata": {
        "id": "g5uGnqlk4eiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence, wv):\n",
        "\n",
        "    sentence_vectors = [wv[word] for word in sentence if word in wv]\n",
        "    if sentence_vectors:\n",
        "        return np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "p05Uzf45ex0g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Function to Find the Top 5 most Similar Sentences from the Dataset based on the Input Sentence**"
      ],
      "metadata": {
        "id": "gdE82ctC4e_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences, k=5):\n",
        "    vec = get_sentence_vector(input_sentence, wv)\n",
        "    if vec is None:\n",
        "        return\n",
        "\n",
        "    similarities = cosine_similarity(vec, np.array(vectorized_sentences))[0]\n",
        "    top_k_idx = similarities.argsort()[::-1][:k]\n",
        "\n",
        "    return [(filtered_sentences[i], similarities[i]) for i in top_k_idx]\n"
      ],
      "metadata": {
        "id": "jiqJNa0gfc-s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Input Sentence**"
      ],
      "metadata": {
        "id": "aiTBPHCf4f15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = [\"president\", \"got\", \"no\", \"money\"]\n",
        "top_matches = find_top_k_similar(input_sentence, wv, vectorized_sentences, filtered_sentences)\n",
        "\n",
        "for i, (sentence, score) in enumerate(top_matches, 1):\n",
        "    print(f\"{i}. {' '.join(sentence)} (Similarity: {score:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6CkZYTzvu1d",
        "outputId": "9e3b179b-578d-4bca-a978-b8530f75cdc8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. guy 's running president want give 'free ' money (Similarity: 0.7809)\n",
            "2. copycat chick-fil-a sandwich recipe ( hungry sunday ) (Similarity: 0.6981)\n",
            "3. clothe organization : family 's closet say ( photo ) (Similarity: 0.6963)\n",
            "4. 'la la land ' win bafta 's top prize , continue hot streak road oscar (Similarity: 0.6809)\n",
            "5. guy 's get 2 word president , 's put d.c . (Similarity: 0.6600)\n"
          ]
        }
      ]
    }
  ]
}